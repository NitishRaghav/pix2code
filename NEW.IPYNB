{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\python\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, RepeatVector, concatenate, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from os import listdir\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images=[]\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    images = []\n",
    "    texts = []\n",
    "    for filename in (all_filenames):\n",
    "        if str(filename).endswith(\"png\"):\n",
    "            # Load the images already prepared in arrays\n",
    "            image = Image.open(data_dir+\"/\"+filename)\n",
    "            image = image.resize((256,256)).convert('RGB')  \n",
    "            images.append(np.array(image).astype('float16')/255.0)\n",
    "        elif str(filename).endswith(\".gui\"):\n",
    "            syntax = '<START> ' + load_doc(data_dir+\"/\"+filename) + ' <END>'\n",
    "            # Separate all the words with a single space\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            # Add a space after each comma\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            texts.append(syntax)\n",
    "        else:\n",
    "            print(f\"File Ignored: {filename}\")\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,sequences = load_data(\"../../github/pix2code/datasets/pix2code_datasets/web/all_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1742, 256, 256, 3), 1742)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape,len(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizr= Tokenizer(filters='', split=\" \", lower=False)\n",
    "Tokenizr.fit_on_texts([load_doc('resources/bootstrap.vocab')])\n",
    "#  add a one spot for the empty word in the vocabulary\n",
    "vocab_size = len(Tokenizr.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = Tokenizr.texts_to_sequences(sequences)\n",
    "max_sequence = max(len(s) for s in train_sequences)\n",
    "\n",
    "max_length = 48 \n",
    "# why max length is 48, but max_sequence is 117?\n",
    "# max_sequence is the maximum length of the sequence in the dataset, while max_length is the length of the sequence that we want to generate.\n",
    "# so basically we are lower the sequences size ?\n",
    "# yes, we are lowering the sequence size to the length of the longest sequence in the dataset.\n",
    "# why do we need to lower the sequence size?\n",
    "# The reason we are lowering the sequence size is because the model can't handle sequences of arbitrary length.\n",
    "# okay Copilot ,cool\n",
    "\n",
    "X, y, image_data = list(), list(), list()\n",
    "for img_no, seq in enumerate(train_sequences):\n",
    "    for i in range(1, len(seq)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        X.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "        image_data.append(train_features[img_no])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the float 64 to lower as this causing the memory issue\n",
    "# /fix MemoryError: Unable to allocate 173. GiB for an array with shape (118259, 256, 256, 3) and data type float64\n",
    "\n",
    "# X = np.array(X).astype('float32')\n",
    "# y = np.array(y).astype('float32')\n",
    "# image_data = np.array(image_data).astype('float16')\n",
    "\n",
    "X_train, X_test, y_train, y_test, image_train, image_test = train_test_split(X, y, image_data, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, RepeatVector, concatenate, Embedding, GRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from os import listdir\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load the document\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images = []\n",
    "    # Load all the files and order them\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if str(filename).endswith(\"png\"):\n",
    "            # Load the images already prepared in arrays\n",
    "            image = Image.open(data_dir+\"/\"+filename)\n",
    "            image = image.resize((256,256)).convert('RGB')  \n",
    "            images.append(np.array(image))\n",
    "        elif filename.endswith(\".gui\"):\n",
    "            syntax = '<START> ' + load_doc(data_dir+\"/\"+filename) + ' <END>'\n",
    "            # Separate all the words with a single space\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            # Add a space after each comma\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            text.append(syntax)\n",
    "        else:\n",
    "            print(f\"Ignoring file: {filename}\")\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, text\n",
    "\n",
    "def data_generator(data_dir, tokenizer, max_sequence, max_length, vocab_size, batch_size=32):\n",
    "    # Load all the files and order them\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    while True:\n",
    "        images = []\n",
    "        texts = []\n",
    "        for filename in (all_filenames):\n",
    "            if str(filename).endswith(\"png\"):\n",
    "                # Load the images already prepared in arrays\n",
    "                image = Image.open(data_dir+\"/\"+filename)\n",
    "                image = image.resize((256,256)).convert('RGB')  \n",
    "                images.append(np.array(image))\n",
    "                if len(images) == batch_size:\n",
    "                    yield preprocess_data(images, texts, tokenizer, max_sequence, max_length, vocab_size)\n",
    "                    images = []\n",
    "                    texts = []\n",
    "            elif filename.endswith(\".gui\"):\n",
    "                syntax = '<START> ' + load_doc(data_dir+\"/\"+filename) + ' <END>'\n",
    "                # Separate all the words with a single space\n",
    "                syntax = ' '.join(syntax.split())\n",
    "                # Add a space after each comma\n",
    "                syntax = syntax.replace(',', ' ,')\n",
    "                texts.append(syntax)\n",
    "        \n",
    "        if images:\n",
    "            yield preprocess_data(images, texts, tokenizer, max_sequence, max_length, vocab_size)\n",
    "\n",
    "def preprocess_data(images, texts, tokenizer, max_sequence, max_length, vocab_size):\n",
    "    X, y, image_data = list(), list(), list()\n",
    "    train_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    for img, seq in zip(images, train_sequences):\n",
    "        for i in range(1, len(seq)):\n",
    "            # Add the sentence until the current count(i) and add the current count to the output\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad all the input token sentences to max_sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
    "            # Turn the output into one-hot encoding\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # Add the corresponding image to the bootstrap token file\n",
    "            image_data.append(img)\n",
    "            # Cap the input sentence to 48 tokens and add it\n",
    "            X.append(in_seq[-48:])\n",
    "            y.append(out_seq)\n",
    "    return np.array(X), np.array(y), np.array(image_data)\n",
    "\n",
    "dir_name = '../../github/pix2code/datasets/pix2code_datasets/web/all_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "train_features, texts = load_data(dir_name)\n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "train_generator = data_generator(dir_name, tokenizer, max_sequence, max_length, vocab_size)\n",
    "\n",
    "# Initialize the function to create the vocabulary \n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# The longest set of bootstrap tokens\n",
    "max_sequence = 50\n",
    "\n",
    "# Specify how many tokens to have in each input sentence\n",
    "max_length = 48\n",
    "\n",
    "# Initialize the function to create the vocabulary \n",
    "train_sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Add one spot for the empty word in the vocabulary \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_features, train_feature_val, text_train, text_val = train_test_split(train_features, train_sequences, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393, 256, 256, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your model architecture\n",
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
    "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "\n",
    "image_model.add(RepeatVector(max_length))\n",
    "\n",
    "visual_input = Input(shape=(256, 256, 3,))\n",
    "encoded_image = image_model(visual_input)\n",
    "\n",
    "language_input = Input(shape=(max_length,))\n",
    "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
    "language_model = GRU(128, return_sequences=True)(language_model)\n",
    "language_model = GRU(128, return_sequences=True)(language_model)\n",
    "\n",
    "decoder = concatenate([encoded_image, language_model])\n",
    "decoder = GRU(512, return_sequences=True)(decoder)\n",
    "decoder = GRU(512, return_sequences=False)(decoder)\n",
    "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
    "optimizer = RMSprop(learning_rate=0.0001, clipvalue=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model for every 2nd epoch\n",
    "filepath=\"/weights/org-weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, save_freq=2)\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 48)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 48, 50)               900       ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " gru (GRU)                   (None, 48, 128)              69120     ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 48, 1024)             1354142   ['input_3[0][0]']             \n",
      "                                                          88                                      \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                 (None, 48, 128)              99072     ['gru[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 48, 1152)             0         ['sequential_1[0][0]',        \n",
      "                                                                     'gru_1[0][0]']               \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                 (None, 48, 512)              2558976   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " gru_3 (GRU)                 (None, 512)                  1575936   ['gru_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 18)                   9234      ['gru_3[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 139727526 (533.02 MB)\n",
      "Trainable params: 139727526 (533.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object data_generator at 0x000002948C87FCA0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit([image_data, X], y, batch_size=35, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=50)\n",
    "# Save the final model\n",
    "model.save('models/org_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# !shutdown.exe /r /t 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
