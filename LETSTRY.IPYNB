{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## integrating data generators with batches to avoid memory issue and trying to fiting this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\python\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Flatten, Dense, Dropout, Input, LSTM, concatenate, RepeatVector, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some issues occuring solved in below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirnamee=r'../../Screenshot-to-code/Bootstrap/resources/datasets/'\n",
    "\n",
    "# def load_doc(file):\n",
    "#     with open(file, 'r') as f:\n",
    "#         text = f.read()\n",
    "#     return text\n",
    "\n",
    "# def load_data(dirname):\n",
    "#     texts = []\n",
    "#     images = []\n",
    "\n",
    "#     # loading all the filenames\n",
    "#     allfiles = listdir(dirname)\n",
    "#     allfiles.sort()\n",
    "#     for filename in allfiles:\n",
    "#         if str(filename).endswith('npz'):\n",
    "#             image = np.load(dirname + filename)\n",
    "#             # images\n",
    "#             images.append(image['features']/255.0)\n",
    "#         else:\n",
    "#             # text\n",
    "#             syntax = '<START> ' + load_doc(dirname + filename) + ' <END>'\n",
    "#             syntax = \" \".join(syntax.split())\n",
    "#             syntax = syntax.replace(',', ' ,')\n",
    "#             texts.append(syntax)\n",
    "\n",
    "#     return np.array(images), texts\n",
    "\n",
    "# # Load the bootstrap tokens\n",
    "# tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "# tokenizer.fit_on_texts([load_doc('bootstrap.vocab')])\n",
    "\n",
    "# # Add one spot for the empty word in the vocabulary \n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Map the input sentences into the vocabulary indexes\n",
    "# train_sequences = tokenizer.texts_to_sequences(load_data(dirname=dirnamee)[1])\n",
    "\n",
    "# # The longest set of bootstrap tokens\n",
    "# max_sequence = max(len(s) for s in train_sequences)\n",
    "\n",
    "# # Specify how many tokens to have in each input sentence\n",
    "# max_length = 48\n",
    "\n",
    "# def preprocess_data(sequences, features):\n",
    "#     X, y, image_data = [], [], []\n",
    "#     for img_no, seq in enumerate(sequences):\n",
    "#         for i in range(1, len(seq)):\n",
    "#             # Add the sentence until the current count(i) and add the current count to the output\n",
    "#             in_seq, out_seq = seq[:i], seq[i]\n",
    "#             # Pad all the input token sentences to max_sequence\n",
    "#             in_seq = pad_sequences([in_seq], maxlen=max_sequence, padding='pre', truncating='post')[0]\n",
    "#             # Turn the output into one-hot encoding\n",
    "#             out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "#             # Add the corresponding image to the bootstrap token file\n",
    "#             image_data.append(features[img_no])\n",
    "#             # Cap the input sentence to 48 tokens and add it\n",
    "#             X.append(in_seq[-max_length:])\n",
    "#             y.append(out_seq)\n",
    "#     return np.array(X, dtype='float16'), np.array(y, dtype='float16'), np.array(image_data, dtype='float16')\n",
    "\n",
    "# # Data generator\n",
    "# def data_generator(dirname, batch_size=32):\n",
    "#     images, texts = load_data(dirname)\n",
    "#     sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "#     while True:\n",
    "#         # Shuffle the data\n",
    "#         indices = np.arange(len(images))\n",
    "#         np.random.shuffle(indices)\n",
    "#         images = images[indices]\n",
    "#         sequences = [sequences[i] for i in indices]\n",
    "        \n",
    "#         for start in range(0, len(images), batch_size):\n",
    "#             end = min(start + batch_size, len(images))\n",
    "            \n",
    "#             X_batch, y_batch, image_data_batch = preprocess_data(sequences[start:end], images[start:end])\n",
    "            \n",
    "#             yield [image_data_batch, X_batch], y_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Flatten, Dense, Dropout, Input, LSTM, concatenate, RepeatVector, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split  # Added import for train-test split\n",
    "dirnamee=r'../../Screenshot-to-code/Bootstrap/resources/datasets/'\n",
    "\n",
    "\n",
    "def load_doc(file):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Load data from the specified directory\n",
    "def load_data(dirname):\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    # Loading all the filenames\n",
    "    allfiles = listdir(dirname)\n",
    "    allfiles.sort()\n",
    "    for filename in allfiles:\n",
    "        if str(filename).endswith('npz'):\n",
    "            image = np.load(dirname + filename)\n",
    "            # Images\n",
    "            images.append(image['features'] / 255.0)\n",
    "        else:\n",
    "            # Text\n",
    "            syntax = '<START> ' + load_doc(dirname + filename) + ' <END>'\n",
    "            syntax = \" \".join(syntax.split())\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            texts.append(syntax)\n",
    "\n",
    "    return np.array(images), texts\n",
    "\n",
    "# Load the bootstrap tokens\n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "tokenizer.fit_on_texts([load_doc('bootstrap.vocab')])\n",
    "\n",
    "# Add one spot for the empty word in the vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Map the input sentences into the vocabulary indexes\n",
    "train_sequences = tokenizer.texts_to_sequences(load_data(dirname=dirnamee)[1])\n",
    "\n",
    "# The longest set of bootstrap tokens\n",
    "max_sequence = max(len(s) for s in train_sequences)\n",
    "\n",
    "# Specify how many tokens to have in each input sentence\n",
    "max_length = 48\n",
    "\n",
    "def preprocess_data(sequences, features):\n",
    "    X, y, image_data = [], [], []\n",
    "    for img_no, seq in enumerate(sequences):\n",
    "        for i in range(1, len(seq)):\n",
    "            # Add the sentence until the current count(i) and add the current count to the output\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad all the input token sentences to max_sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_sequence, padding='pre', truncating='post')[0]\n",
    "            # Turn the output into one-hot encoding\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # Add the corresponding image to the bootstrap token file\n",
    "            image_data.append(features[img_no])\n",
    "            # Cap the input sentence to 48 tokens and add it\n",
    "            X.append(in_seq[-max_length:])\n",
    "            y.append(out_seq)\n",
    "    return np.array(X, dtype='float16'), np.array(y, dtype='float16'), np.array(image_data, dtype='float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data generator with validation split\n",
    "def data_generator(dirname, batch_size=32):\n",
    "    images, texts = load_data(dirname)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle the data\n",
    "        indices = np.arange(len(images))\n",
    "        np.random.shuffle(indices)\n",
    "        images = images[indices]\n",
    "        sequences = [sequences[i] for i in indices]\n",
    "        \n",
    "        # Calculate the split point for validation data (20%)\n",
    "        split_point = int(0.2 * len(images))\n",
    "        val_images, val_sequences = images[:split_point], sequences[:split_point]\n",
    "        \n",
    "        for start in range(split_point, len(images), batch_size):\n",
    "            end = min(start + batch_size, len(images))\n",
    "            \n",
    "            X_batch, y_batch, image_data_batch = preprocess_data(sequences[start:end], images[start:end])\n",
    "            \n",
    "            yield [image_data_batch, X_batch], y_batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator with validation split\n",
    "def val_generator(dirname, batch_size=32):\n",
    "    images, texts = load_data(dirname)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle the data\n",
    "        indices = np.arange(len(images))\n",
    "        np.random.shuffle(indices)\n",
    "        images = images[indices]\n",
    "        sequences = [sequences[i] for i in indices]\n",
    "        \n",
    "        # Calculate the split point for validation data (20%)\n",
    "        split_point = int(0.2 * len(images))\n",
    "        val_images, val_sequences = images[:split_point], sequences[:split_point]\n",
    "        \n",
    "        for start in range(split_point, len(images), batch_size):\n",
    "            end = min(start + batch_size, len(images))\n",
    "            \n",
    "            X_batch, y_batch, image_data_batch = preprocess_data(sequences[start:end], images[start:end])\n",
    "            \n",
    "            yield [image_data_batch, X_batch], y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples: 1\n",
      "Number of batches: 0\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined your val_generator\n",
    "# with the appropriate parameters (dirname and batch_size)\n",
    "\n",
    "# Get the validation data generator\n",
    "val_gen = val_generator(dirname='1190Data/', batch_size=32)\n",
    "\n",
    "# Calculate the total number of validation samples\n",
    "total_val_samples = len([3])  # Assuming val_sequences contains your validation data\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = total_val_samples // 32\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total validation samples: {total_val_samples}\")\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "print(f\"Batch size: {32}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2288"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(next(val_generator(dirname='1190Data/', batch_size=32))[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\python\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the encoder\n",
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(128, 128, 3,)))\n",
    "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "\n",
    "image_model.add(RepeatVector(max_length))\n",
    "\n",
    "visual_input = Input(shape=(128, 128, 3,))\n",
    "encoded_image = image_model(visual_input)\n",
    "\n",
    "language_input = Input(shape=(max_length,))\n",
    "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
    "language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "\n",
    "# Create the decoder\n",
    "decoder = concatenate([encoded_image, language_model])\n",
    "decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
    "optimizer = RMSprop(learning_rate=0.0001, clipvalue=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 32\n",
    "# train_generator = data_generator(dirname=dirnamee, batch_size=batch_size)\n",
    "# steps_per_epoch = len(train_sequences) // batch_size\n",
    "\n",
    "# # Define the checkpoint\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# checkpoint_path = \"model_weights_epoch_{epoch:02d}.h5\"\n",
    "# checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "#                              save_weights_only=True, \n",
    "#                              save_freq='epoch', \n",
    "#                              period=2)\n",
    "\n",
    "# # Fit the model with validation data\n",
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model with batches\n",
    "batch_size = 32\n",
    "train_generator = data_generator(dirname=dirnamee, batch_size=batch_size)\n",
    "val_generator = val_generator(dirname='1190Data/', batch_size=batch_size)\n",
    "steps_per_epoch = len(train_sequences) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Define the checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint_path = \"model_weights_epoch_{epoch:02d}.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             save_weights_only=True, \n",
    "                             save_freq='epoch', \n",
    "                             period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitish.raghav\\AppData\\Local\\Temp\\ipykernel_15196\\2209798556.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, validation_data=val_generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From f:\\python\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "40/54 [=====================>........] - ETA: 25:56 - loss: 2.3863"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.11.5' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, validation_data=val_generator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
